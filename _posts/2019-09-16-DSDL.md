# 20. 자연어 처리

그들은 언어의 축제에서 찌꺼기를 훔쳐왔다. - 윌리엄 셰익스피어

자연어 처리는 언어에 대한 계산적 기술 (computational techniques)의 집합이다. 넓은 분야.

## 워드 클라우드

관심사 관련된 단어의 수

축이 가지는 의미에 대해 고민. 보통 의미가 없을 거 같다.

```python
def text_size(total):
  """total이 0이라면 8을 반환, 200이라면 28을 반환"""
  return 8 + total / 200 * 20

for word, job_popularity, resume_popularity in data:
  plt.txt(job_popularity, resume_popularity, word, ha='center', va='center',size = text_size(job_popularity + resume_popularity))
  
plt.xlabel("popularity on job postings")
plt.ylabel("popularity on resumes")
plt.xticks([])
plt.yticks([])
plt.show()
```



## n-gram 모델

프로그래밍을 통해 웹사이트를 만들어 낼 방법이 없는지 궁금해 한다. = 언어를 모델링할 방법을 찾아야 한다.

한 가지 방법은 문서가 여러 개 있는 말뭉치, corpus를 구해서 언어에 대한 통계적 모델을 만드는 것이다. 여기서는 Mike Loukides의 에세이 '데이터 과학이란 무엇인가'에서 출발

데이터를 수집하기 위해서 requests와 BeautifulSoup을 이용

**주의사항**

- 텍스트 안에 있는 작은 따옴표들이 사실은 유니코드 문자 u'\u2019'라는 것. 유니코드 따옴표를 일반 아스키 따옴표로 변환시켜 주는 간단한 함수를 만든다

  ```python
  def fix_unicode(text):
    return text.replace(u"\u2019", "'")
  ```

- 웹페이지의 텍스트를 얻고 나서 텍스트를 단어와 마침표로 나눠줘야 한다 (여기서 마침표를 보존하는 것은 문장이 어디서 끝나는지 알기 위함이다). re.findall()을 사용하면 간단하게 구현할 수 있다.

  ```python
  from bs4 import BeautifulSoup
  import requests
  url = "http://radar.oreilly.com/2010/06/what-is-data-science.html"
  html = request.get(url).txt
  soup = BeautifulSoup(html, 'html5lib')
  
  content = soup.find("div", "entry-content") # entry-content div 를 찾아줌
  regex = r"[\w']+|[\.] # 단어나 마침표에 해당하는 문자열
  
  documents = []
  
  for paragraph in content("p"):
    words = re.findall(regex, fix_unicode(paragraph.text))
    document.extend(words)물론 이보다 데이터를 더 깔끔하게 정제할 수 도 있다. 
  ```


물론 이보다 데이터를 더 깔끔하게 정제할 수도 있다. 장과 절의 제목을 제거하거나 "Web2.0"같은 문자열의 중간은 끊는다든지, 이미지에 딸린 캡션이 나 주석을 제거하는 방법등이 있다. 하지만 이러한 전처리는 한도 끝도 없다.

텍스트를 단어의 열로 만들었으니, 이제 다음과 같은 방법으로 언어를 모델링할 수 있다. 먼저, 시작하는 단어를 하나 준다. 여기서는 이 단어를 "book"이라고 해보자. 그리고 문서들을 주욱 흝으면서, "book"이라는 단어 다음에 등장하는 단어들이 무엇인지 확인한다. 여기서는 "isn't", "a", "shows", "demonstrates", "teaches"등의 단어가 등장했다. 이 중에서 " book" 다음으로 등장할 법한 단어를 임의로 선택하고, 문장의 끝을 의미하는 마침표가 등장할 때까지 이 과정을 무한히 반복한다. 이 과정을 2-gram 또는 바이그램(bigram) 모델이라고 부른다. 바이그램 모델은 데이터에 등장하는 바이그램의 빈도를 통해 언어를 모델링 한다.

그렇다면 시작 단어는 어떻게 정할까? 마침표 다음에 등장하는 단어 중에서 임의로 하나 고르는 것도 방법

각 단어 뒤에 어떤 단어가 따라오는지 알아보자. zip은 입력되는 list 중에서 먼저 끝나는 쪽에 맞춰서 종료된다는 것을 기억하는가? 이 성직을 이용하면 document에서 순차적으로 등장하는 단어들에 관한 정보를 정확하게 얻을 수 있다.

```python
bigrams = zip(document, document[1:])
transitions = defaultdict(list)
for prev, current in bigrams:
  transitions[prev].append[current]
```

이제 문장을 생성할 준비가 되었다.

```python
def generate_using_bigrams():
  current = "." # 이 뜻은 다음 단어가 문장의 시작이라는 것
  result = []
  while True:
    next_word_candidates = transitions[current] # bigrams (current, _)
    current = random.choice(next_word_candidates) # 임의로 하나를 선택
    result.append(current)
    if current == ".": return " ".join(result)  
```

여기서 생성한 문장은 터무니 없는 것들이지만, 데이터 과학과 관련되어 보일 법한 웹사이트를 만들 때 사용할 만한 것들이기도 하다. 다음과 같은 문구를 생성할 수 있다.

*If you may know which are you want to data sort the data feeds web friend someone on trending topics as the data in Hadoop is the data science requires a book demonstrates why visualizations are but we ......*

2개의 연속적인 단어를 보는 바이그램 모델이 아니라 3개의 연속적인 단어를 보는 트라이그램(trigram) 모델을 사용하면 문장은 더욱 그럴듯한 형태가 된다. 트라이그램 모델에서는 직전 두 개의 단어에 의해 다음 단어가 결정된다.

```python
trigrams = zip(document, document[1:], document[2:])
trigram_transitions = defaultdict(list)
starts = []

for prev, current, next in trigrams:
  if prev == ".": 	# 만약 이전 단어가 마침표였다면
    starts.append(current) # 이제 새로운 단어의 시작을 의미한다
  trigram_transitions[(prev, current)].append(next)

```

시작 단어를 따로 직접 찾아봐야 한다. 문장은 바이그램과 비슷한 방식으로 생성할 수 있다.

```python
def generate_using_trigram():
  current = random.choice(starts) # 임의의 시작 단어를 선택
  prev = "." # 앞에 '.'를 추가
  result = [current]
  while True:
    next_word_candidates = trigram_transitions[(prev, current)]
    next_word = random.choice(next_word_candidates)
    
    prev, current = current, next_word
    result.append(current)
    
    if current == ".":
      return " ".join(result)
```

이제 조금 더 괜찮은 문장들이 나오는 것을 확인할 수 있다.

*In hindsight MapReduce seems like an epidemic and if so does that give us new insights into how economics work That's not a question we could even have asked a few years there has been instrumented.*

트라이그램을 사용하면 다음 단어를 생성하는 각 단계에서 선택할 수 있는 단어의 수가 바이그램을 사용할 때 보다 훨씬 적어졌고, 선택할 수 있는 단어가 딱 하나만 존재하는 경우도 많았을 것이다. 즉, 이미 어떤 문서 상에 존재했던 문장 (또는 적어도 긴 문구 ) 하나를 그대로 생성했을 수도 있다. 이 경우 더 많은 데이터를 사용하는 것이 도움. 이는 데이터 과학에 관한 더 많은 에세이를 모으고, 이를 토대로 n-gram 모델을 구축하는 것을 의미한다.

## 문법

언어 모델을 구축하는 또 다른 방법은 문법에 기반하여 말이 되는 문장을 생성하는 것이다. 품사 (part of speech)란 무엇이며, 그들을 어떻게 조합하면 문장이 되는지 기억. 만일 당신의 영어 선생님이 영 별로였다면, 명사 다음에는 항상 동사가 따른다고 배웠을 것이다. 그런 방식으로 문장을 생성할 수 있기는 하지만, 여기서는 약간 더 복잡한 문법을 만들어 볼 것이다.

```python
grammar = {
  "_S" : ["_NP _VP"],
  "_NP" : ["_N",
          "_A _NP _P _A _N",],
  "_VP" : ["_V",
          "_V _NP"],
  "_N" : ["data science", "Python", "regression"],
  "_A" : ["big", "linear", "logistic"],
  "_P" : ["about", "near"],
  "_V" : ["learns", "trains", "tests", "is"]    
}
```

여기서 항목 앞에 밑줄이 있으면 더 확장할 수 있는 규칙(rule)이고, 나머지는 종결어(terminal)라 하자.

예를 들어, '_S'는 문장 규칙을 의미하며 '\_NP'라는 명사구 (noun phrase) 규칙과 연이어 '\_VP 동사구(verb phrase)' 규칙이 등장하는 규칙을 생성해 준다.

동사구 규칙은 '\_V'라는 동사 규칙, 또는 동사 규칙과 명사구 규칙으로 이어지는 규칙을 생성할 수 있다.

한편 '_NP' 규칙은 자기 자신을 포함할 수도 있다. 이렇게 문법 규칙들은 얼마든지 재귀적일 수 있기 때문에, 유한 문법 (finite grammar)도 무한히 많은 문장을 생성할 수 있게 된다.

문장 규칙 ["_S"] 로 시작한다고 했을 때, 이를 대체할 수 있는 항목 중에서 임의로 한 가지를 선택하고, 모든 항목이 종결어가 되기 전까지 이 과정을 반복한다. 예를 들어 다음과 같은 과정이 있을 수 있다.

먼저, 특정 항목이 종결어인지 아닌지 판단하는 함수가 필요하다.

```python
def is_terminal(token):
  return token[0] != "_"
```

다음으로 각 항목을 대체 가능한 다른 항목 또는 항목들로 변환시키는 함수가 필요. 이 함수에서는 가장 먼저 등장하는 규칙 항목이 무엇인지를 찾는다. 규칙 항목을 찾을 수 없다면 모든 항목이 종결어로 구성되어 있다는 것을 의미하므로 함수를 종료하면 되고, 규칙 항목을 찾는다면 그것을 대체할 수 있는 여러 항목 중 하나를 임의로 선택한다. 이때 선택된 항목이 종결어, 즉 단어라면 기존 항목을 단순히 대체하기만 하면 된다. 한편 선택된 항목이 종결어가 아니라면, 공백으로 구분된 여러 항목을 분할해서 그 항목들로 기존 항목을 대체해야 한다.

이 과정을 반복하는 코드는 다음과 같이 작성할 수 있다.

```python
def expand(grammar, tokens):
  for i, token in enumerate(tokens):
    # 종결어는 건너뛰고
    if is_terminal(token): continue
     
    # 만약 여기로 왔다면 종결어가 아닌 단어를 찾은 것이다
    # 대체할 수 있는 항목을 임의로 선택
    replacement = random.choice(grammar[token])
    
    if is_terminal(replacement):
      token[i] = replacement
    else:
      tokens = tokens[:i] + replacement.split() + tokens[(i+1):]
      
    # 새로운 단어의 list에 expand를 적용
    return expand(grammar, tokens)
  
  # 이제 모든 단어가 종결어이기 때문에 종료
  return tokens      
```

이제 문장을 생성할 수 있다.

```python
def generate_sentence(grammar):
  return expand(grammar, ["_S"])
```

문법을 바꿔보기도 하고... 단어를 더하거나, 규칙을 더하거나, 품사를 추가해도..

한편, 문법은 문장을 생성할 때 보다 그 반대 방향으로 적용할 때 더 흥미롭기도 하다. 즉, 문장에 어떤 요소가 있는지 알아보기 위해 문장을 파싱(parsing)할 때 문법을 사용할 수 있다. 이를 통해 어떤 부분이 목적어이고 어떤 부분이 동사인지 알 수 있기 때문에 문장의 으미를 파악하는 데 도움을 준다.

데이터 과학을 이용해 텍스트를 생성하는 것은 근사한 일이지만, 텍스트를 이해하는 기술은 더 매력적이다.

## 여담: 깁스 샘플링

몇몇 확률분포로부터는 데이터를 손쉽게 샘플링할 수 있다. 예를 들어 균등분포로부터 샘플링하는 것은 쉽다.

```python
random.random()
```

정규분포로부터의 샘플링도.. *inverse_normal_cdf(random.random())*

하지만 다른 복잡한 확률분포로부터 데이터를 샘플링하는 것은 간단한 작업이 아니다. 깁스 샘플링(Gibbs sampling) 은 이때 사용할 수 있는 기술 중 하나인다.

깁스 샘플링이란, 일부 조건부 확률분포만 알고 있을 때 다차원 분포로부터 표본을 얻을 수있는 방법이다.

주사위 두 개를 던진다고 해보자. 첫 번째 주사위의 눈을 x, 두 주사위의 눈을 더한 값을 y라고 한다면 (x,y) 쌍에 대한 표본은 다음과 같이 간단하게 생성할 수 있다.

```python
def roll_a _die():
  return random.choice([1,2,3,4,5,6,])

def direct_sample():
  d1 = roll_a_die()
  d2 = roll_a_die()
  return d1, d1 + d2
```

하지만 우리가 아는 것이 조건부 확률분포뿐이라고 하자. x가 주어졌을 때 y의 분포를 아는 것은 쉽다. x의 값을 알고 있다는 전제 하에 y는 x+1, x+2, x+3, x+4, x+5, x+6 중 한 가지 값을 가질 것이기 때문이다.

```python
def random_y_given_x(x):
  """x+1, x+2, ... , x+6가 될 확률은 동일"""
  return x + roll_a_die()
```

하지만 반대 방향 즉, y 값을 알고 있다는 전제하에 x의 값을 알아내는 것은 조금 더 복잡하다. y가 2라면 x는 반드시 1이겠지만, y가 3이라면 x는 1 또는 2다, 마찬가지로 y가 11이라면 x는 5 또는 6이다.

```python
def random_x_given_y(y):
  if y<=7:
    # 만약 총 합이 7이거나 7보다 작다면
    # 첫 번째 주사위는 모두 동일한 확률로 1, 2, ..., (total - 1)일 것이다
    return random.randrange(1, y)
  else:
    # 만약 총 합이 7이거나 7보다 크다면
    # 첫 번째 주사위는 모두 동일한 확률로 (total - 6), (total - 5), ..., 6 일 것이다
    return random.randrange(y-6, 7)
```

깁스 샘플링은, 임의의 (유효한) x 또는 y 값에서 출발해서 x에 대한 y의 조건부 확률과 y에 대한 x의 조건부 확률 사이를 오가며 반복적으로 값을 선택하는 방법이다.

```python
def gibbs_sampling(num_iters=100):
  x, y = 1, 2 # 값이 무엇이든 상관이 없다
  for _in range(num_iters):
    x = random_x_given_y(y)
    y = random_y_given_x(x)
  return x, y
```

이렇게 하면 실제로, 원래 분포에서 샘플링했을 때와 유사한 결과값이 나오는 것을 확인할 수 있다.

```python
def compare_distributions(num_samples=1000):
  counts = defaultdict(lambda: [0, 0])
  for _ in range(num_samples):
    counts[gibbs_sample()][0] += 1
    counts[direct_sample()][1] += 1
  return counts
```

## 토픽 모델링

관심사가 일치하는 것. 추천기 섬세한 접근법은, 각 관심사와 관련된 '토픽'이 무엇인지 파악. 이 접근법은 Latent Dirichlet Allocation(LDA) 라고 부르며, 여러 문서가 있을 때 문서들 간 공통적인 토픽을 파악할 때 종종 사용한다.

사용자의 관심사가 문서라고 가정하고, LDA 적용

LDA는 문서에 대한 확률분포를 가정한다는 점에서 *나이브베이즈 분류기*와 비슷하기도 하다. 모델에 대한 가정은 다음과 같다

- 토픽의 수는 K개로 고정되어 있다
- 각 토픽을 단어에 대한 확률분포로 나타내어주는 확률변수가 존재한다. 이 분포는 토픽 k가 주어졌을 때, 단어 w를 관찰할 수 있는 확률로 생각하면 편하다.
- 각 문서를 토픽에 대한 확률분포로 나타내주는 확률변수가 존재한다. 이 분포는 문서 d에 어떤 토픽들이 섞여 있는지를 나타내는 분포로 생각하면 편하다.
- 문서 내의 각 단어는 문서의 토픽 분포로부터 먼저 임의의 토픽이 선택된 뒤, 토픽의 단어 분포로부터 생성되었다고 가정한다.

먼저, 문서의 집합 documents가 있고, 각 문서는 단어들의 list로 구성되어 있다고 하자, 이 경우 document_topics는 0부터 K-1 사이의 숫자로 표기된 토픽들로 각 문서의 개별 단어에 할당된다.

이에 따르면 4번째 문서의 5번째 단어는 다음과 같으며, 

```python
documents[3][4]
```

그 단어를 생성한 토픽은 다음과 같다

```python
document_topics[3][4]
```

이는 각 문서의 토픽 분포를 멱시적으로 나타냄과 동시에, 각 토픽의 단어 분포를 암시적으로 나타내 준다.

토픽 1이 특정 단어를 생성할 likelihood는 토픽 1이 그 단어를 생성하는 횟수와 임의의 단어를 생성하는 횟수를 비교함으로써 알 수 있다.

토픽을 단순한 숫자로 표기했지만, 각 토픽에서 큰 영향을 끼치는 단어들을 보고 각 토픽에 의미 있는 이름을 부여할 수도 있다. 어떻게든 document_topics를 생성하기만 하면 되는데, 이런 경우 깁스 샘플링이 유용하게 쓰인다.

먼저 모든 문서의 모든 단어에 임의의 토픽을 부여하는 것으로 시작한다. 그리고 각 문서의 각 단어를 하나씩 살펴보며, 현재 문서의 토픽 분포와 현재 토픽의 단어 분포에 따라 각 토픽에 weight를 할당한다. 그다음에는 그 weight를 사용해서 해당 단어에 알맞은 새로운 토픽을 할당한다. 이 과정을 여러 번 반복하면, 토픽/단어 분포와 문서/토픽 분포의 결합확률분포로부터 나오는 표본을 얻게 된다.

일단 랜덤으로 생성된 weight들을 이용해서 인덱스를 생성하는 부분은 다음과 같다.

```python
def sample_from(weights):
  """i를 weights[i] / sum(weights)의 확률로 변환"""
  total = sum(weights)
  rnd = total * random.random() # 0과 total 사이를 균일하게 선택
  for i, w in enumerate(weights):
    rnd -=w # 밑의 주석의 식을 만족하는 가장 작은 i를 반환
    if rnd <=0: return i
```

예를 들어, weight가 [1, 1, 3]라면, 

1/5의 확률로 0, 

1/5의 확률로 1,

3/5의 확률로 2 를 반환하게 된다.

문서가 다음과 같이 사용자의 관심사를 나타낼 때,

```python
documents = [
  ["Hadoop", "Big Data", "HBase", "Java", "Spark", "Storm", "Cassandra"],
  ["NoSQL", "MongoDB", "Cassandra", "HBase", "Postgres"],
  ["Python", "scikit-learn", "scipy", "numpy","statsmodels", " pandas"],
  
]
```

총 K=4개의 토픽을 반환해 보자.

weight를 계산하기 위해서는 먼저 몇 가지 숫자를 알고 있어야 한다. 일단 숫자를 실제로 세기 이전에, 숫자들을 담을 자료 구조를 만들어 보자.

- 각 토픽이 각 문서에 할당되는 횟수:
  - \# Counter로 구성된 list
  - \# 각의 Counter는 각 문서를 의미함
  - document_topic_counts = [Counter() for _ in documents]